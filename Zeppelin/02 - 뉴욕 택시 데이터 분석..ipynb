{
  "metadata": {
    "name": "02 - 뉴욕 택시 데이터 분석",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nfrom pyspark.sql import SparkSession\nspark \u003d SparkSession.builder.appName(\u0027trip_count_sql\u0027).getOrCreate()\nspark"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n# 데이터가 들어있는 디렉토리 지정\ndirectory \u003d \"/home/ubuntu/working/datasource\"\n\ntrips_files \u003d \"trips/*\"\nzone_file \u003d \"taxi+_zone_lookup.csv\""
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\ntrips_df \u003d spark.read.csv(f\"file://{directory}/{trips_files}\", inferSchema\u003dTrue, header\u003dTrue)\nzone_df \u003d spark.read.csv(f\"file://{directory}/{zone_file}\", inferSchema\u003dTrue, header\u003dTrue)"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\ntrips_df.printSchema()\nzone_df.printSchema()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# DataLake -\u003e Data Warehouse\n\n- 필요한 데이터를 JOIN\n- 적절한 전처리 (dataframe을 만들기 위한 과정)\n- 필요한 데이터만 걸러내는 과정"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\ntrips_df.createOrReplaceTempView(\"trips\")\nzone_df.createOrReplaceTempView(\"zone\")"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nquery \u003d \"\"\"\nSELECT * FROM trips\n\"\"\"\nz.show(spark.sql(query))"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nquery \u003d \"\"\"\nSELECT * FROM zone\n\"\"\"\nz.show(spark.sql(query))"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n# Warehouse 생성 쿼리\nquery \u003d \"\"\"\n    SELECT\n        t.VendorID as vendor_id,\n        TO_DATE(t.tpep_pickup_datetime) as pickup_date,\n        TO_DATE(t.tpep_dropoff_datetime) as dropoff_date,\n        HOUR(t.tpep_pickup_datetime) as pickup_time,\n        HOUR(t.tpep_dropoff_datetime) as dropoff_time,\n        \n        t.passenger_count,\n        t.trip_distance,\n        t.fare_amount,\n        t.tip_amount,\n        t.tolls_amount,\n        t.total_amount,\n        t.payment_type,\n        \n        pz.Zone as pickup_zone,\n        dz.Zone as dropoff_zone\n    FROM trips t\n    \n    LEFT JOIN zone pz ON t.PULocationID \u003d pz.locationID\n    LEFT JOIN zone dz ON t.DOLocationID \u003d dz.locationID\n\"\"\"\n\ncomb_df \u003d spark.sql(query)\nz.show(comb_df)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Data Warehouse -\u003e Mart\n- Warehouse에서 Mart를 만들기 위해 데이터를 검사 및 정제"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\ncomb_df.createOrReplaceTempView(\"comb\")\n"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# 1. 날짜와 시간\n\nquery \u003d \"\"\"\n    SELECT\n        pickup_date, pickup_time\n    FROM comb\n    WHERE pickup_time \u003e\u003d 0\n    ORDER BY pickup_date\n\"\"\"\n\nz.show(spark.sql(query))"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nquery \u003d \"\"\"\nSELECT count(*)\nFROM comb\nWHERE pickup_date \u003c \u00272021-01-01\u0027\n\"\"\"\n\nz.show(spark.sql(query))"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# 2. 요금 데이터 확인 - 통계 정보 활용\n\ncomb_df_describe \u003d comb_df.select(\"total_amount\").describe()\nz.show(comb_df_describe)"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# 3. 거리 데이터 확인 - 통계 정보\n\ncomb_df_distance \u003d comb_df.select(\u0027trip_distance\u0027).describe()\n\nz.show(comb_df_distance)"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# 4. 월별 운행수 확인\n\nquery \u003d \"\"\"\n    SELECT\n        DATE_TRUNC(\"MM\", pickup_date) as month,\n        COUNT(*) as trips\n    FROM comb\n    GROUP BY month\n    ORDER BY month DESC\n\"\"\"\n\nz.show(spark.sql(query))"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# 5. 승객에 대한 통계정보 확인\n\ncomb_df_passenger_cnt \u003d comb_df.select(\"passenger_count\").describe()\nz.show(comb_df_passenger_cnt)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Warehouse -\u003e Mart\n- 살펴본 내용을 토대로 실제 분석할 데이터로 정제.\n"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n# 데이터 정제\nquery \u003d \"\"\"\n\nSELECT *\nFROM comb c\nWHERE c.total_amount \u003c 200\n  AND c.total_amount \u003e 0\n  AND c.passenger_count \u003c 5\n  AND c.pickup_date \u003e\u003d \u00272021-01-01\u0027\n  AND c.pickup_date \u003c \u00272021-08-01\u0027\n  AND c.trip_distance \u003c 10\n  AND c.trip_distance \u003e 0\n\"\"\"\n# mart - 데이터 분석 준비 완료\ncleaned_df \u003d spark.sql(query)\ncleaned_df.createOrReplaceTempView(\"cleaned\")"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nz.show(cleaned_df.describe())"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# EDA\n"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# pickup_date 별 운행수 확인\n\nquery \u003d \"\"\"\n    SELECT\n        pickup_date,\n        COUNT(pickup_date) as trips\n    FROM cleaned\n    GROUP BY pickup_date\n    ORDER BY pickup_date ASC\n\"\"\"\n\nz.show(spark.sql(query))"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# 요일별 운행 수 확인\n\nquery \u003d \"\"\"\n    SELECT\n        DATE_TRUNC(\"MM\", pickup_date) as month,\n        DATE_FORMAT(pickup_date, \u0027EEEE\u0027) as day_of_week,\n        COUNT(*) as trips\n    FROM cleaned\n    GROUP BY month, day_of_week\n    ORDER BY month, day_of_week\n\"\"\"\n\nz.show(spark.sql(query))"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# 결제 유형 분석\n\nz.show(cleaned_df.select(\u0027payment_type\u0027))"
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n# 숫자인 결제 타입을 문자열로\ndef parse_payment_type(payment_type):\n\n    payment_type_to_string \u003d {\n      1: \"Credit Card\",\n      2: \"Cash\",\n      3: \"No Charge\",\n      4: \"Dispute\",\n      5: \"Unknown\",\n      6: \"Voided Trip\",\n    }\n\n    return payment_type_to_string[payment_type]"
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nspark.udf.register(\u0027parse_payment_type\u0027, parse_payment_type)"
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n# 결제 타입별 통계\n\nquery \u003d \"\"\"\n    SELECT\n        parse_payment_type(payment_type) as payment_type_str,\n        COUNT(*) as trips,\n        MEAN(fare_amount) as mean_fare_amount,\n        STD(fare_amount) as std_fare_amount\n    FROM cleaned\n    GROUP BY payment_type\n\"\"\"\n\nz.show(spark.sql(query))"
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nspark.stop()"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n"
    }
  ]
}