{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b370fdd1",
   "metadata": {},
   "source": [
    "# UDF (User Defined Function)\n",
    "- 사용자 정의 함수\n",
    "- Spark dataframe, SQL에서 사용 가능\n",
    "- 사용자가 만든 함수를 Worker의 Task에서 사용 가능하게"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc79aee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/11/17 10:24:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://0.0.0.0:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>udf</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f8ad8e51940>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('udf').getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b70d9f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions = [\n",
    "    ('찹쌀탕수육+짜장2', '2021-11-07 13:20:00', 22000, 'KRW'),\n",
    "    ('등심탕수육+크립새우+짜장면', '2021-10-24 11:19:00', 21500, 'KRW'), \n",
    "    ('월남 쌈 2인 세트', '2021-07-25 11:12:40', 42000, 'KRW'), \n",
    "    ('콩국수+열무비빔국수', '2021-07-10 08:20:00', 21250, 'KRW'), \n",
    "    ('장어소금+고추장구이', '2021-07-01 05:36:00', 68700, 'KRW'), \n",
    "    ('족발', '2020-08-19 19:04:00', 32000, 'KRW'),  \n",
    "]\n",
    "\n",
    "schema = [\"name\", \"datetime\", \"price\", \"currency\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15e02693",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[name: string, datetime: string, price: bigint, currency: string]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame(data=transactions, schema=schema)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aca11af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('menus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9fc00a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+-------------------+-----+--------+\n",
      "|                      name|           datetime|price|currency|\n",
      "+--------------------------+-------------------+-----+--------+\n",
      "|          찹쌀탕수육+짜장2|2021-11-07 13:20:00|22000|     KRW|\n",
      "|등심탕수육+크립새우+짜장면|2021-10-24 11:19:00|21500|     KRW|\n",
      "|          월남 쌈 2인 세트|2021-07-25 11:12:40|42000|     KRW|\n",
      "|       콩국수+열무비빔국수|2021-07-10 08:20:00|21250|     KRW|\n",
      "|       장어소금+고추장구이|2021-07-01 05:36:00|68700|     KRW|\n",
      "|                      족발|2020-08-19 19:04:00|32000|     KRW|\n",
      "+--------------------------+-------------------+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "    SELECT *\n",
    "    FROM menus\n",
    "\"\"\"\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6611b564",
   "metadata": {},
   "source": [
    "- UDF는 분산 병렬 처리 환경에서 사용할 수 있는 함수 만들떄 사용. (Worker에서 작동)\n",
    "- return type을 따로 지정하지 않으면 string이 default\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33a6a8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def squared(n):\n",
    "    return n * n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7132af4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.squared(n)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import LongType\n",
    "\n",
    "# register('Worker에서 사용할 함수 이름', 마스터에 정의된 함수의 이름, return type)\n",
    "spark.udf.register('squared', squared, LongType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e51a92e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 2:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+\n",
      "|price|squared(price)|\n",
      "+-----+--------------+\n",
      "|22000|     484000000|\n",
      "|21500|     462250000|\n",
      "|42000|    1764000000|\n",
      "|21250|     451562500|\n",
      "|68700|    4719690000|\n",
      "|32000|    1024000000|\n",
      "+-----+--------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "    SELECT price, squared(price)\n",
    "    FROM menus\n",
    "\"\"\"\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8513bfc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_number(n):\n",
    "    units = [\"\", \"십\", \"백\", \"천\", \"만\"]\n",
    "    nums = '일이삼사오육칠팔구'\n",
    "    result = []\n",
    "    i = 0\n",
    "    while n > 0:\n",
    "        n, r = divmod(n, 10)\n",
    "        if r > 0:\n",
    "            result.append(nums[r-1]+units[i])\n",
    "        i += 1\n",
    "    return \"\".join(reversed(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98c128a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'사만팔천칠백'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_number(48700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "546729da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.read_number(n)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.udf.register('read_number', read_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "048552e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+\n",
      "|price|read_number(price)|\n",
      "+-----+------------------+\n",
      "|22000|          이만이천|\n",
      "|21500|      이만일천오백|\n",
      "|42000|          사만이천|\n",
      "|21250|  이만일천이백오십|\n",
      "|68700|      육만팔천칠백|\n",
      "|32000|          삼만이천|\n",
      "+-----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "    SELECT price, read_number(price)\n",
    "    FROM menus\n",
    "\"\"\"\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2be65296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|       A/5 21171|   7.25| null|       S|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|        PC 17599|71.2833|  C85|       C|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|female|26.0|    0|    0|STON/O2. 3101282|  7.925| null|       S|\n",
      "|          4|       1|     1|Futrelle, Mrs. Ja...|female|35.0|    1|    0|          113803|   53.1| C123|       S|\n",
      "|          5|       0|     3|Allen, Mr. Willia...|  male|35.0|    0|    0|          373450|   8.05| null|       S|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filepath = \"/home/ubuntu/working/spark-examples/data/titanic_train.csv\"\n",
    "titanic_sdf = spark.read.csv(filepath, inferSchema=True, header=True)\n",
    "\n",
    "titanic_sdf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "649f2514",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "avg_age = titanic_sdf.select(F.avg(F.col('Age')))\n",
    "avg_age_row = avg_age.head()\n",
    "avg_age_value = avg_age.head()[0]\n",
    "\n",
    "# Spark DataFrame의 fillna()에 인자로 Dict를 입력하여 여러개의 컬럼들에 대해서 결측치 값을 입력할 수 있게 만들어줌. \n",
    "titanic_sdf_filled = titanic_sdf.fillna({'Age': avg_age_value, \n",
    "                                         'Cabin': 'C000',\n",
    "                                         'Embarked': 'S'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a99895bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 나이의 카테고리를 구하는 함수를 정의\n",
    "def get_category(age):\n",
    "    cat = ''\n",
    "    \n",
    "    if age <= 5: cat = 'Baby'\n",
    "    elif age <= 12: cat = 'Child'\n",
    "    elif age <= 18: cat = 'Teenager'\n",
    "    elif age <= 25: cat = 'Student'\n",
    "    elif age <= 35: cat = 'Young Adult'\n",
    "    elif age <= 60: cat = 'Adult'\n",
    "    else : cat = 'Elderly'\n",
    "    \n",
    "    return cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "742a2df5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.get_category(age)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "\n",
    "udf_get_category = F.udf(get_category, StringType())\n",
    "udf_get_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "48d1d24c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------+\n",
      "|              Age|AgeCategory|\n",
      "+-----------------+-----------+\n",
      "|             22.0|    Student|\n",
      "|             38.0|      Adult|\n",
      "|             26.0|Young Adult|\n",
      "|             35.0|Young Adult|\n",
      "|             35.0|Young Adult|\n",
      "|29.69911764705882|Young Adult|\n",
      "|             54.0|      Adult|\n",
      "|              2.0|       Baby|\n",
      "|             27.0|Young Adult|\n",
      "|             14.0|   Teenager|\n",
      "|              4.0|       Baby|\n",
      "|             58.0|      Adult|\n",
      "|             20.0|    Student|\n",
      "|             39.0|      Adult|\n",
      "|             14.0|   Teenager|\n",
      "|             55.0|      Adult|\n",
      "|              2.0|       Baby|\n",
      "|29.69911764705882|Young Adult|\n",
      "|             31.0|Young Adult|\n",
      "|29.69911764705882|Young Adult|\n",
      "+-----------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "titanic_sdf_filled_category = titanic_sdf_filled.withColumn('AgeCategory', udf_get_category(F.col('Age')))\n",
    "titanic_sdf_filled_category.select('Age', 'AgeCategory').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7df35e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee606894",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
